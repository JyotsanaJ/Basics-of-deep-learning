{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence #(1)\n",
    "\n",
    "def custom_collate(data): #(2)\n",
    "    inputs = [torch.tensor(d['tokenized_input']) for d in data] #(3)\n",
    "    labels = [d['label'] for d in data]\n",
    "\n",
    "    inputs = pad_sequence(inputs, batch_first=True) #(4)\n",
    "    labels = torch.tensor(labels) #(5)\n",
    "\n",
    "    return { #(6)\n",
    "        'tokenized_input': inputs,\n",
    "        'label': labels\n",
    "    }\n",
    "\n",
    "loader = DataLoader(nlp_data, batch_size=2, shuffle=False, collate_fn=custom_collate) #(7)\n",
    "\n",
    "iter_loader = iter(loader)\n",
    "batch1 = next(iter_loader)\n",
    "pprint(batch1)\n",
    "batch2 = next(iter_loader)\n",
    "pprint(batch2)\n",
    "\n",
    "# {'label': tensor([0, 0]),\n",
    "#  'tokenized_input': tensor([\n",
    "#   [  1,   4,   5,   9,   3,   2,   0,   0,   0],\n",
    "#   [  1,   7,   3,  14,  48,   7,  23, 154,   2]\n",
    "# ])}\n",
    "\n",
    "# {'label': tensor([1, 0]),\n",
    "#  'tokenized_input': tensor([\n",
    "#   [  1,  30,  67, 117,  21,  15,   2],\n",
    "#   [  1,  17,   2,   0,   0,   0,   0]])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the whole dataset to the longest example.\n",
    "\n",
    "Pad dynamically during batch creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data with collate_fn()\n",
    "\n",
    "# def collate_fn(data: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "#     tensors, targets = zip(*data)\n",
    "#     features = pad_sequence(tensors, batch_first=True)\n",
    "#     targets = torch.stack(targets)\n",
    "#     return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(data: List[Dict[str, str]], tokenizer: PreTrainedTokenizer) -> Dict[str, torch.Tensor]:\n",
    "    # Ensure tokenizer has a padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Extract and concatenate informal and formal statements\n",
    "    sequences = []\n",
    "    for idx, example in enumerate(data):\n",
    "        # Handle null values\n",
    "        informal = example.get(\"generated informal statement\", \"\") or \"\"\n",
    "        formal = example.get(\"formal statement\", \"\") or \"\"\n",
    "\n",
    "        # Skip if both are empty\n",
    "        if not informal and not formal:\n",
    "            continue\n",
    "\n",
    "        sequences.append(f'informal statement {informal} formal statement {formal}')\n",
    "\n",
    "    # Tokenize the sequences\n",
    "    tokenized_data = tokenizer(sequences, padding='longest', truncation=True, return_tensors='pt')\n",
    "    tokenized_data[\"labels\"] = tokenized_data[\"input_ids\"].clone()\n",
    "\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [train_dataset[i] for i in range(batch_size)]\n",
    "processed_data = custom_collate_fn(sample_data, tokenizer=tokenizer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
