{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Models\n",
    "\n",
    "1. Encoders-Decoders\n",
    "2. Attentions\n",
    "3. transformers\n",
    "4. Transfer Learning - 2018\n",
    "5. LLms - Chatgpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018-2020 - Vision transformers\n",
    "\n",
    "2021 - genAI\n",
    "\n",
    "2022 - Chatgpt / stable diffusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT (Encoder only architecture) - (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "* Decision to use an encoder-only architecture in BERT suggests a primary emphasis on understanding input sequences rather than generating output sequences.\n",
    "* 12 layer of encoders.\n",
    "* Large FFN and large attention heads.\n",
    "* BERT base as 110M parameters to train.\n",
    "* Takes [CLS] as 1st token.\n",
    "* input token 512 at one go.\n",
    "* Model output of size 786 dimension.\n",
    "\n",
    "* WordPiece Embedding is a subword tokenization algorithm used in natural language processing (NLP) tasks. \n",
    "* It breaks down words into smaller units called subword tokens, allowing machine learning models to better handle out-of-vocabulary (OOV) words and improve performance on various NLP tasks.\n",
    "* “geeksforgeeks” can be split into “geeks” “##for”, and”##geeks”. The “##” prefix indicates that the subword is a continuation of the previous one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage:\n",
    "1. Classification.\n",
    "2. Question answering\n",
    "3. NER\n",
    "4. Text Summarization\n",
    "5. Semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT is Pretrained on 2 tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Masked Language Model (MLM)\n",
    "* Before BERT learns from sentences, it hides some words (about 15%) and replaces them with a special symbol, like [MASK]\n",
    "* BERT adds a special layer on top of its learning system to make these guesses. It then checks how close its guesses are to the actual hidden words.\n",
    "* BERT’s main focus during training is on getting these hidden words right. It cares less about predicting the words that are not hidden.\n",
    "* of the 15% hidden words. 80% are masked, 10% is wrongly replaced and 10% not replaced.\n",
    "* We do this to make sure that model learns that masked words can be wrong but not always. Its not always hidden only. It can be right also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Next Sentence Prediction (NSP)\n",
    "* BERT predicts if the second sentence is connected to the first. \n",
    "* In the training process, BERT learns to understand the relationship between pairs of sentences, predicting if the second sentence follows the first in the original document.\n",
    "* done by transforming the output of the [CLS] token into a 2×1 shaped vector using a classification layer, and then calculating the probability of whether the second sentence follows the first using SoftMax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\",\n",
    "                                            unk_token=\"[UNK]\",\n",
    "                                            sep_token=\"[SEP]\",\n",
    "                                            pad_token=\"[PAD]\",\n",
    "                                            cls_token=\"[CLS]\",\n",
    "                                            mask_token=\"[MASK]\",\n",
    "                                            never_split=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [101, 24705, 1204, 17095, 1942, 1110, 170, 1846, 2235, 1872, 1118, 3353, 1592, 2240, 117, 1359, 1113, 1103, 15175, 1942, 113, 9066, 15306, 11689, 118, 3972, 13809, 23763, 114, 4220, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "text = 'ChatGPT is a language model developed by OpenAI, based on the GPT (Generative Pre-trained Transformer) architecture. '\n",
    "\n",
    "# Tokenize and encode the text\n",
    "encoding = tokenizer.encode(text, max_length=512)\n",
    "print(\"Token IDs:\", encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 24705, 1204, 17095, 1942, 1110, 170, 1846, 2235, 1872, 1118, 3353, 1592, 2240, 117, 1359, 1113, 1103, 15175, 1942, 113, 9066, 15306, 11689, 118, 3972, 13809, 23763, 114, 4220, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]']\n"
     ]
    }
   ],
   "source": [
    "# Convert token IDs back to tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens([encoding[0]])\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 436M/436M [02:03<00:00, 3.54MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Load the basic BERT model \n",
    "bert_model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-cased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.37.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Sentiment Classifier class \n",
    "class SentimentClassifier(nn.Module):\n",
    "    \n",
    "    # Constructor class \n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    \n",
    "    # Forward propagaion class\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "        #  Add a dropout layer \n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model and move to classifier\n",
    "model = SentimentClassifier(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bert_model.config.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations \n",
    "EPOCHS = 10\n",
    "\n",
    "# Optimizer Adam \n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Set the loss function \n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 - Encoder- decoder \n",
    "* T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks.\n",
    "* The input of the encoder is the corrupted sentence.\n",
    "* the input of the decoder is the original sentence and the target is then the dropped out tokens delimited by their sentinel tokens.\n",
    "* Teacher Forcing.\n",
    "* cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 2.32k/2.32k [00:00<00:00, 4.30MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 1.57MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 3.74MB/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 3.42MB/s]\n",
      "model.safetensors: 100%|██████████| 242M/242M [00:48<00:00, 5.04MB/s] \n",
      "generation_config.json: 100%|██████████| 147/147 [00:00<00:00, 477kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome to NYC', 'HuggingFace is a company']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_source_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "# Suppose we have the following 2 training examples:\n",
    "input_sequence_1 = \"Welcome to NYC\"\n",
    "output_sequence_1 = \"Bienvenue à NYC\"\n",
    "\n",
    "input_sequence_2 = \"HuggingFace is a company\"\n",
    "output_sequence_2 = \"HuggingFace est une entreprise\"\n",
    "\n",
    "# encode the inputs\n",
    "task_prefix = \"translate English to French: \"\n",
    "input_sequences = [input_sequence_1, input_sequence_2]\n",
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[13959,  1566,    12,  2379,    10,  5242,    12, 13465,     1,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [13959,  1566,    12,  2379,    10, 11560,  3896,   371,  3302,    19,\n",
       "             3,     9,   349,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer(\n",
    "    [task_prefix + sequence for sequence in input_sequences],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_source_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[10520, 15098,     3,    85, 13465,     1,     0,     0],\n",
       "        [11560,  3896,   371,  3302,   259,   245, 11089,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the targets\n",
    "target_encoding = tokenizer(\n",
    "    [output_sequence_1, output_sequence_2],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_target_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "target_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10520, 15098,     3,    85, 13465,     1,  -100,  -100],\n",
       "        [11560,  3896,   371,  3302,   259,   245, 11089,     1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = target_encoding.input_ids\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "labels[labels == tokenizer.pad_token_id] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18801367282867432"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Bienvenue à NYC</s><pad><pad>\n",
      "Bienvenue à NYC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyotsana/codebase/DeepLearning/venv1/lib/python3.11/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y = model.generate(input_ids)\n",
    "print(tokenizer.decode(y[0]))\n",
    "print(tokenizer.decode(y[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "#set up tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small', return_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_piece_sequence = (\"The series focuses on Monkey D. Luffy, a young man made of rubber, who, inspired by his childhood idol,\" \n",
    "             \"the powerful pirate Red-Haired Shanks, sets off on a journey from the East Blue Sea to find the mythical treasure,\" \n",
    "             \"the One Piece, and proclaim himself the King of the Pirates. In an effort to organize his own crew, the Straw Hat Pirates,\" \n",
    "             \"Luffy rescues and befriends a pirate hunter and swordsman named Roronoa Zoro, and they head off in search of the \" \n",
    "             \"titular treasure. They are joined in their journey by Nami, a money-obsessed thief and navigator; Usopp, a sniper \"\n",
    "             \"and compulsive liar; and Sanji, a perverted but chivalrous cook. They acquire a ship, the Going Merry, and engage in confrontations\"  \n",
    "             \"with notorious pirates of the East Blue. As Luffy and his crew set out on their adventures, others join the crew later in the series, \"\n",
    "             \"including Tony Tony Chopper, an anthropomorphized reindeer doctor; Nico Robin, an archaeologist and former Baroque Works assassin; \"\n",
    "             \"Franky, a cyborg shipwright; Brook, a skeleton musician and swordsman; and Jimbei, a fish-man helmsman and former member of the Seven \"\n",
    "             \"Warlords of the Sea. Once the Going Merry is damaged beyond repair, Franky builds the Straw Hat Pirates a new ship, the Thousand Sunny,\" \n",
    "             \"Together, they encounter other pirates, bounty hunters, criminal organizations, revolutionaries, secret agents, and soldiers of the\" \n",
    "             \"corrupt World Government, and various other friends and foes, as they sail the seas in pursuit of their dreams.\")\n",
    "inputs = tokenizer.encode(\"summarize: \" + one_piece_sequence,\n",
    "                          return_tensors='pt',\n",
    "                          max_length=512,\n",
    "                          truncation=True)\n",
    "summarization_ids = model.generate(inputs, max_length=80, min_length=40, length_penalty=5., num_beams=2)\n",
    "summarization = tokenizer.decode(summarization_ids[0])\n",
    "print(summarization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## translate English to French\n",
    "\n",
    "language_sequence = (\"You should definitely watch 'One Piece', it is so good, you will love the comic book\")\n",
    "input_ids = tokenizer(\"translate English to French: \"+language_sequence, return_tensors=\"pt\").input_ids \n",
    "language_ids = model.generate(input_ids)\n",
    "language_translation = tokenizer.decode(language_ids[0],skip_special_tokens=True)\n",
    "print(language_translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sentence Similarity\n",
    "\n",
    "stsb_sentence_1 = (\"Luffy was fighting in the war.\")\n",
    "stsb_sentence_2 = (\"Luffy's fighting style is comical.\")\n",
    "input_ids = tokenizer(\"stsb sentence 1: \"+stsb_sentence_1+\" sentence 2: \"+stsb_sentence_2, return_tensors=\"pt\").input_ids \n",
    "stsb_ids = model.generate(input_ids)\n",
    "stsb = tokenizer.decode(stsb_ids[0],skip_special_tokens=True)\n",
    "print(stsb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Lite BERT (ALBERT)\n",
    "\n",
    "* Parameters compressions\n",
    "* 1.7X faster than BERT\n",
    "* ALBERT requires much more computations due to its longer structures.\n",
    "* ALBERT is suited better for problems when the speed can be traded off for achieving higher accuracy.\n",
    "* Embedding size - 128.\n",
    "* Hidden Units - [768, 1024, 2048, 4096]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  RoBERTa (Robustly Optimized BERT-Pretraining Approach) (META AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* doesn’t use the next-sentence pretraining objective.\n",
    "* Is trained with much larger mini-batches and learning rates.\n",
    "* Uses a byte-level BPE tokenizer.\n",
    "* Trained on 160GB of uncompressed text.\n",
    "* RoBERTa is trained for longer sequences\n",
    "* Training with dynamic masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    " \n",
    "\n",
    "#Loading the model and tokenizer\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "#Tokenizing the input\n",
    "inputs = tokenizer(\"I love my cat\", return_tensors=\"pt\")\n",
    " \n",
    "\n",
    "#Retrieving the logits and using them for predicting the underlying emotion\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT (OpenAI’s Generative Pre-trained Transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT1-2\n",
    "* semi-supervision.\n",
    "* Decoder only model.\n",
    "* learned positional encoding.\n",
    "* This results in an embedding which contains information about the word, and where the word is in the sequence.\n",
    "* autoregressive generation.\n",
    "* 1.5B parameter Transformer - GPT2\n",
    "\n",
    "### GPT3\n",
    "* 175 billion parameters - GPT3\n",
    "* GPT-3 can get pretty good at most tasks without training the model.\n",
    "* Instead you can just experiment with the input to the model to find the right input format for a particular task. \n",
    "\n",
    "* And thus, PROMPT ENGINEERING was born.\n",
    "\n",
    "### GPT4 \n",
    "* Uses Mixture of Experts (MOE). Instead of using a whole model. Hae parts of model dedicated to different domains like art, science and use only that or load only that.\n",
    "* needs to use 280 billion parameters of the models 1.8 Trillion (a mere 15%) on any given inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q ipywidgets torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pipeline\n",
    "\n",
    "Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i/p : 100 tokens\n",
    "embedding dim : 512\n",
    "\n",
    "i/p embedding: 100 * 512\n",
    "\n",
    "Positional embedding: 100 * 512\n",
    "\n",
    "Multihead = 8\n",
    "\n",
    "self attention \n",
    "Default weight initialization??\n",
    "\n",
    "If multiple head we divide 512/8\n",
    "\n",
    "Wq, Wk, Wv = [512 * 64]\n",
    "\n",
    "i/p = 100 * 512\n",
    "\n",
    "q = [100*64] \n",
    "k = [100*64]\n",
    "\n",
    "Attention scores = [100*100]\n",
    "q.k\n",
    "[100*64].[100*64]T = [100*100] Attention scores\n",
    "\n",
    "Scaling it down to avoid overflow for attention scores sqrt(key length):\n",
    "\n",
    "[100*100] scaled down\n",
    "\n",
    "softmax => [100*100]  probabilities\n",
    "\n",
    "v * attention_scores\n",
    "[100*100] * [100*64] = [100 * 64] concat 7[100*64] = [100*512] from multihead attention\n",
    "\n",
    "[100*512] Multihead attention layer \n",
    "Layer normalization - Length of inpute sentence \n",
    "Residual connections - Avoid vanishing gradients"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
